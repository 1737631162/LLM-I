# 概念
与传统模型不同，LLM为了更好地捕捉信息，区别不同位置的token，必须将位置信息融入到输入或者更改attention的实现，使其可以区分不同位置的token。

# 绝对位置编码
- 训练式  
  将位置编码作为训练参数，比如最大长度为512，编码维度为768，那么就初始化一个512×768的矩阵作为位置向量，缺点是没有外推性，只能处理max_seq_len=512的句子
  
- 三角式（最常见，来自Attention Is All You Need）
$$
\left\{\begin{array}{l}\boldsymbol{p}_{k, 2 i}=\sin \left(k / 10000^{2 i / d}\right) \\ \boldsymbol{p}_{k, 2 i+1}=\cos \left(k / 10000^{2 i / d}\right)\end{array}\right.
$$
  
- 递归式
  RNN模型不需要位置编码，它在结构上就自带了学习到位置信息的可能性，因为其递归结构$p_{k+1}=f(p_k)$可以一定程度编码位置
  


# 相对位置编码
使用相对位置信息去替换展开式的某一项
$$
\left\{\begin{aligned} \boldsymbol{q}_{i} & =\left(\boldsymbol{x}_{i}+\boldsymbol{p}_{i}\right) \boldsymbol{W}_{Q} \\ \boldsymbol{k}_{j} & =\left(\boldsymbol{x}_{j}+\boldsymbol{p}_{j}\right) \boldsymbol{W}_{K} \\ \boldsymbol{v}_{j} & =\left(\boldsymbol{x}_{j}+\boldsymbol{p}_{j}\right) \boldsymbol{W}_{V} \\ a_{i, j} & =\operatorname{softmax}\left(\boldsymbol{q}_{i} \boldsymbol{k}_{j}^{\top}\right) \\ \boldsymbol{o}_{i} & =\sum_{j} a_{i, j} \boldsymbol{v}_{j}\end{aligned}\right.
$$  
$$
\boldsymbol{q}_{i} \boldsymbol{k}_{j}^{\top}=\left(\boldsymbol{x}_{i}+\boldsymbol{p}_{i}\right) \boldsymbol{W}_{Q} \boldsymbol{W}_{K}^{\top}\left(\boldsymbol{x}_{j}+\boldsymbol{p}_{j}\right)^{\top}=\left(\boldsymbol{x}_{i} \boldsymbol{W}_{Q}+\boldsymbol{p}_{i} \boldsymbol{W}_{Q}\right)\left(\boldsymbol{W}_{K}^{\top} \boldsymbol{x}_{j}^{\top}+\boldsymbol{W}_{K}^{\top} \boldsymbol{p}_{j}^{\top}\right)
$$

  

# 旋转位置编码
目前主流大模型均采用此编码方式  
将q和k映射到复平面 
如果我们将$q_m$,$k_n$ 分别乘以$e^{imθ}$,$e^{inθ}$ 变成$q_m e^{imθ}$,$k_n e^{inθ}$，那么就相当于给它们配上了绝对位置编码了（因为显式地依赖绝对位置m,n），然后放到内积里边，我们有$<q_m e^{imθ},k_n e^{inθ}> = Re[(q_m e^{imθ})(q_n e^{inθ})^*]=Re[q_m k_n^* e^{i(m-n)θ}]$

以复数向量内积的形式运算包含了绝对和相对位置信息，最终内积只依赖于相对位置m-n，这将绝对位置与相对位置融合在一起。

将其拓展至2D向量对于位置为n的二维实数向量[x,y]，我们当它复数来运算，乘以$e^{inθ}$，得到恒等式$(x+iy)e^{inθ}=(xcosnθ-ysinnθ)+i(ycosnθ+xsinnθ)$

这也就是意味着，通过
$\left(\begin{matrix}x \\y\end{matrix} \right) => \left(\begin{matrix}xcosnθ-ysinnθ \\xsinnθ+yconsnθ\end{matrix}\right) = \left(\begin{matrix}x \\y\end{matrix}\right) cosnθ + \left(\begin{matrix}-y \\x\end{matrix}\right) sinnθ$

来赋予[x,y]绝对位置信息，那么在Attention运算的时候也等价于相对位置编码。如果是多于二维的向量，可以考虑每两维为一组进行同样的运算，每一组的θ可以不一样。

