# 训练过程
1. 预训练：首先，语言模型通过大量的文本数据进行预训练，学习到语言的基本结构和常识。  
2. 监督微调（Supervised Fine-Tuning, SFT）：接下来，使用带有标注的数据集对预训练模型进行微调，使其能够完成特定的任务或生成符合要求的文本。  
3. 奖励建模（Reward Modeling）：PPO在这个阶段，收集人类对不同输出质量的偏好数据，并训练一个奖励模型来预测哪些输出更受欢迎。这个奖励模型会为后续的强化学习步骤提供必要的奖励信号。  
4. 强化学习优化：最后，利用奖励模型提供的奖励信号，采用对齐优化算法对SFT模型进一步优化。以PPO举例，PPO是一种策略优化方法，它通过与环境交互并根据奖励调整策略来最大化预期奖励。具体来说，在PPO训练过程中，模型生成多个可能的输出，然后使用奖励模型评估这些输出以得到奖励分数，接着使用这些奖励来更新模型参数，使其倾向于生成奖励更高的输出。  

强化学习概念  
通过与环境的交互学习最优决策，Reward是强化学习的核心概念之一，它是指环境给予智能体的反馈信号，用于衡量某个动作的好坏程度。


![image](https://github.com/user-attachments/assets/8cf90c55-5c05-499d-86ad-92921dbd08b7)
# reward
这个奖励模型用于评估生成文本的质量，并为策略优化提供必要的奖励信号。 （PPO和GRPO都需要提前训练奖励模型） 
数据集包括prompt、chosen、rejected
![image](https://github.com/user-attachments/assets/61aff466-e4a4-4afa-a33c-652ea60566b1)  
完成奖励模型的训练后来到最后一步，强化学习阶段（也就是模型对齐阶段）。  

# PPO 
依赖于一个与策略模型大小相当的价值网络来估计函数。这个价值网络需要在每个时间步对状态进行评估，计算复杂度高，内存占用大。  
使用广义优势估计（GAE）来计算优势函数，需要对每个动作的即时奖励和未来奖励的折扣总和进行估计。  
优点：每次训练只能小幅调整，稳定可控，适合复杂任务
缺点：需依赖“评估师”（Critic网络）和大量训练数据，计算成本高。

# DPO
不需要提前训练奖励模型，DPO直接利用人类偏好的数据集来比较模型输出的不同结果，然后选择更符合人类偏好的结果作为优化目标。与PPO不同，DPO不依赖于传统的奖励函数，而是通过直接优化策略来匹配人类的偏好。    
优点：无需训练奖励模型，显存占用低，适合快速微调。  
缺点：依赖高质量偏好数据（若老师批改不准确，学生可能学偏）。

# GRPO
完全摒弃了价值网络，通过组内相对奖励来估计优势函数。
与PPO训练奖励模型的方法不同，虽然都包括奖励模型，但其是通过组间相对优势（Group Relative Advantage）计算奖励：生成多个候选回答，计算组内平均奖励，仅保留奖励显著高于均值的样本用于训练。

优势函数计算  
对于一个prompt，我们生成num_samples个样本，分别计算其奖励，计算平均奖励作为基准值（替代价值函数），然后减去平均奖励，我们就能得到每个样本的优势函数，然后对其进行标准化。  

优点：显存占用降低，适合资源受限场景；多候选生成增强多样性。  
缺点：推理时需生成多个候选答案，耗时增加。


训练数据格式  
- PPO  
可以与奖励模型采用同一数据集
  
- DPO  
![image](https://github.com/user-attachments/assets/2a5e097e-5da7-4967-b8a6-68eb7e1431b3)

  
- GRPO
![image](https://github.com/user-attachments/assets/fff628a0-e04e-4b62-8633-fb675be9cda8)


# 对比
### **关键对比**
| 算法   | 奖励模型需求 | 核心优化目标                     | 计算成本       | 适用场景                     |
|--------|--------------|----------------------------------|----------------|------------------------------|
| **PPO** | 必需         | 最大化奖励模型给出的收益           | 高（需3阶段）  | 复杂任务（如对话系统） |
| **DPO** | 无需         | 直接优化偏好数据的对数似然        | 中（2阶段）    | 答案明确的任务（如问答）|
| **GRPO**| 与传统RM不同，无需训练 | 组间相对优势最大化                | 低（1阶段+规则）| 数学推理、逻辑任务 |

### **计算成本对比表**
| 算法   | 训练阶段数量 | 核心优化复杂度       | 数据需求（偏好/规则） 
|--------|--------------|----------------------|-----------------------|
| **PPO** | 4（预训练→SFT→RM→PPO） | 高（双约束+多迭代） | 10万条偏好数据    |
| **DPO** | 3（预训练→SFT→DPO）   | 中（单目标+单迭代） | 1万条偏好数据        |
| **GRPO**| 2（预训练→SFT+GRPO）  | 低（规则+组内对比） | 0规则或少量合成数据  |
