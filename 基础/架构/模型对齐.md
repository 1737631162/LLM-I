# 训练过程
1. 预训练：首先，语言模型通过大量的文本数据进行预训练，学习到语言的基本结构和常识。  
2. 监督微调（Supervised Fine-Tuning, SFT）：接下来，使用带有标注的数据集对预训练模型进行微调，使其能够完成特定的任务或生成符合要求的文本。  
3. 奖励建模（Reward Modeling）：PPO在这个阶段，收集人类对不同输出质量的偏好数据，并训练一个奖励模型来预测哪些输出更受欢迎。这个奖励模型会为后续的强化学习步骤提供必要的奖励信号。  
4. 强化学习优化：最后，利用奖励模型提供的奖励信号，采用对齐优化算法对SFT模型进一步优化。以PPO举例，PPO是一种策略优化方法，它通过与环境交互并根据奖励调整策略来最大化预期奖励。具体来说，在PPO训练过程中，模型生成多个可能的输出，然后使用奖励模型评估这些输出以得到奖励分数，接着使用这些奖励来更新模型参数，使其倾向于生成奖励更高的输出。  

强化学习概念  
通过与环境的交互学习最优决策，Reward是强化学习的核心概念之一，它是指环境给予智能体的反馈信号，用于衡量某个动作的好坏程度。


![image](https://github.com/user-attachments/assets/8cf90c55-5c05-499d-86ad-92921dbd08b7)
# reward
这个奖励模型用于评估生成文本的质量，并为策略优化提供必要的奖励信号。 （PPO和GRPO都需要提前训练奖励模型） 
数据集包括prompt、chosen、rejected
![image](https://github.com/user-attachments/assets/61aff466-e4a4-4afa-a33c-652ea60566b1)  
完成奖励模型的训练后来到最后一步，强化学习阶段（也就是模型对齐阶段）。  

# PPO（Proximal Policy Optimization）
依赖于一个与策略模型大小相当的价值网络来估计函数。这个价值网络需要在每个时间步对状态进行评估，计算复杂度高，内存占用大。  
使用广义优势估计（GAE）来计算优势函数，需要对每个动作的即时奖励和未来奖励的折扣总和进行估计。  
优点：每次训练只能小幅调整，稳定可控，适合复杂任务
缺点：需依赖“评估师”（Critic网络）和大量训练数据，计算成本高。

# DPO（Direct Preference Optimization）
不需要提前训练奖励模型，DPO直接利用人类偏好的数据集来比较模型输出的不同结果，然后选择更符合人类偏好的结果作为优化目标。与PPO不同，DPO不依赖于传统的奖励函数，而是通过直接优化策略来匹配人类的偏好。    
优点：无需训练奖励模型，显存占用低，适合快速微调。  
缺点：依赖高质量偏好数据（若老师批改不准确，学生可能学偏）。

# GRPO（Group Relative Policy Optimization）
完全摒弃了价值网络，通过组内相对奖励来估计优势函数。
与PPO训练奖励模型的方法不同，虽然都包括奖励模型，但其是通过组间相对优势（Group Relative Advantage）计算奖励：生成多个候选回答，计算组内平均奖励，仅保留奖励显著高于均值的样本用于训练。

优势函数计算  
对于一个prompt，我们生成num_samples个样本，分别计算其奖励，计算平均奖励作为基准值（替代价值函数），然后减去平均奖励，我们就能得到每个样本的优势函数，然后对其进行标准化。  

优点：显存占用降低，适合资源受限场景；多候选生成增强多样性。 零标注依赖：规则可基于合成数据或人工定义，无需标注偏好数据。
缺点：推理时需生成多个候选答案，耗时增加。

示例：
```
### ** 组间相对优势的计算流程**
#### **步骤1：生成候选回答组**
- **输入**：一个提示词（Prompt）`x`。
- **生成**：策略模型（如SFT模型）生成 `K` 个候选回答（例如 `K=5`），构成一个组 `G = {y_1, y_2, ..., y_K}`。
- **示例**：  
  Prompt: "解方程 3x + 5 = 20"  
  组内回答：  
  `y_1: x = 5`  
  `y_2: x = 15/3`  
  `y_3: x = 5`  
  `y_4: 无解`  
  `y_5: 3x = 15 → x = 5`

#### **步骤2：计算组内奖励**
- **规则基奖励函数**：根据任务定义简单规则，为每个回答 `y_i` 计算奖励 `r(y_i)`。  
  - **数学题示例**：  
    - 规则1：答案是否为整数（如 `x=5` → `r=1`，`x=15/3` → `r=0.5`）  
    - 规则2：逻辑是否自洽（如 `y_4: 无解` → `r=0`）  
    - 规则3：答案是否与参考答案一致（如参考答案是 `x=5`，则 `r=1`，否则 `r=0`）  
  - **代码生成示例**：  
    - 规则：代码是否能通过静态类型检查（如Python语法错误 → `r=0`）。

#### **步骤3：确定相对优势阈值**
- **组内平均奖励**：计算组内所有回答的奖励均值 `μ = (1/K) * Σ_{i=1}^K r(y_i)`。
- **相对优势阈值**：  
  - **硬阈值**：仅保留奖励高于均值 `μ` 的回答（如 `r(y_i) > μ`）。  
  - **软阈值**：按奖励排序，保留前 `T%` 的回答（如 `T=20%`，保留奖励最高的1个回答）。

#### **步骤4：筛选训练样本**
- **保留样本**：将满足 `r(y_i) > μ` 或排名前 `T%` 的回答 `y_i` 加入训练数据集。
- **丢弃样本**：其他回答不再用于训练，避免污染模型。

#### **步骤5：优化策略模型**
- **目标函数**：最大化策略模型对保留样本的生成概率，同时通过**分界剪辑（Bounded Clipping）**约束策略更新幅度：  
  \[
  \mathcal{L}_{\text{GRPO}} = -\mathbb{E}_{(x,y)\in D_{\text{advantage}}}\left[\log \pi_\theta(y|x) \cdot \text{Advantage}(y|x)\right]
  \]
  - `D_{\text{advantage}}`：筛选后的高质量样本集。
  - `Advantage(y|x)`：基于规则奖励的动态优势值（如 `r(y) - μ`）。
```

训练数据格式  
- PPO  
可以与奖励模型采用同一数据集
  
- DPO  
![image](https://github.com/user-attachments/assets/2a5e097e-5da7-4967-b8a6-68eb7e1431b3)

  
- GRPO
![image](https://github.com/user-attachments/assets/fff628a0-e04e-4b62-8633-fb675be9cda8)


# 对比
### **关键对比**
| 算法   | 奖励模型需求 | 核心优化目标                     | 计算成本       | 适用场景                     |
|--------|--------------|----------------------------------|----------------|------------------------------|
| **PPO** | 必需         | 最大化奖励模型给出的收益           | 高（需3阶段）  | 复杂任务（如对话系统） |
| **DPO** | 无需         | 直接优化偏好数据的对数似然        | 中（2阶段）    | 答案明确的任务（如问答）|
| **GRPO**| 与传统RM不同，无需训练 | 组间相对优势最大化                | 低（1阶段+规则）| 数学推理、逻辑任务 |

### **计算成本对比表**
| 算法   | 训练阶段数量 | 核心优化复杂度       | 数据需求（偏好/规则） 
|--------|--------------|----------------------|-----------------------|
| **PPO** | 4（预训练→SFT→RM→PPO） | 高（双约束+多迭代） | 10万条偏好数据    |
| **DPO** | 3（预训练→SFT→DPO）   | 中（单目标+单迭代） | 1万条偏好数据        |
| **GRPO**| 2（预训练→SFT+GRPO）  | 低（规则+组内对比） | 0规则或少量合成数据  |
