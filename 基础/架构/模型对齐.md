# 训练过程
1. 预训练：首先，语言模型通过大量的文本数据进行预训练，学习到语言的基本结构和常识。  
2. 监督微调（Supervised Fine-Tuning, SFT）：接下来，使用带有标注的数据集对预训练模型进行微调，使其能够完成特定的任务或生成符合要求的文本。  
3. 奖励建模（Reward Modeling）：在这个阶段，收集人类对不同输出质量的偏好数据，并训练一个奖励模型来预测哪些输出更受欢迎。这个奖励模型会为后续的强化学习步骤提供必要的奖励信号。  
4. 强化学习优化：最后，利用奖励模型提供的奖励信号，采用对齐优化算法对SFT模型进一步优化。以PPO举例，PPO是一种策略优化方法，它通过与环境交互并根据奖励调整策略来最大化预期奖励。具体来说，在PPO训练过程中，模型生成多个可能的输出，然后使用奖励模型评估这些输出以得到奖励分数，接着使用这些奖励来更新模型参数，使其倾向于生成奖励更高的输出。  

强化学习概念  
通过与环境的交互学习最优决策，Reward是强化学习的核心概念之一，它是指环境给予智能体的反馈信号，用于衡量某个动作的好坏程度。


![image](https://github.com/user-attachments/assets/8cf90c55-5c05-499d-86ad-92921dbd08b7)
# reward
数据集包括prompt、chosen、rejected
![image](https://github.com/user-attachments/assets/61aff466-e4a4-4afa-a33c-652ea60566b1)  
完成奖励模型的训练后来到最后一步，强化学习阶段（也就是模型对齐阶段）。  

# PPO 
依赖于一个与策略模型大小相当的价值网络来估计函数。这个价值网络需要在每个时间步对状态进行评估，计算复杂度高，内存占用大。  
使用广义优势估计（GAE）来计算优势函数，需要对每个动作的即时奖励和未来奖励的折扣总和进行估计。  
优点：每次训练只能小幅调整，稳定可控，适合复杂任务
缺点：需依赖“评估师”（Critic网络）和大量训练数据，计算成本高。

# DPO
主要用于从人类偏好数据中直接学习策略。与PPO不同，DPO不依赖于传统的奖励函数，而是通过直接优化策略来匹配人类的偏好。  
优点：无需训练奖励模型，显存占用低，适合快速微调。  
缺点：依赖高质量偏好数据（若老师批改不准确，学生可能学偏）。

# GRPO
完全摒弃了价值网络，通过组内相对奖励来估计优势函数。这种方法通过比较同一状态下的多个动作的奖励值来计算相对优势，显著减少了计算和存储需求。  
通过采样一组动作并计算它们的奖励值，然后对这些奖励值进行归一化处理，得到相对优势。这种方法更直接，减少了对复杂奖励模型的依赖。  
优点：显存占用降低，适合资源受限场景；多候选生成增强多样性。  
缺点：推理时需生成多个候选答案，耗时增加。


训练数据格式  
- PPO  
可以与奖励模型采用同一数据集
  
- DPO  
![image](https://github.com/user-attachments/assets/2a5e097e-5da7-4967-b8a6-68eb7e1431b3)

  
- GRPO
![image](https://github.com/user-attachments/assets/fff628a0-e04e-4b62-8633-fb675be9cda8)

```python
import re
from datasets import load_dataset

SYSTEM_PROMPT = """Respond in the following format:
<reasoning>
  ...
</reasoning>
<answer>
  ...
</answer>"""

def extract_xml_answer(text: str) -> str:
    return text.split("<answer>")[-1].split("</answer>")[0].strip()

def extract_hash_answer(text: str) -> str | None:
    return text.split("####")[-1].strip() if "####" in text else None

def get_gsm8k(split: str = "train"):
    ds = load_dataset("openai/gsm8k", "main", split=split)

    def _map(example):
        return {
            "prompt": [
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": example["question"]},
            ],
            "answer": extract_hash_answer(example["answer"]),
        }

    return ds.map(_map)
```
