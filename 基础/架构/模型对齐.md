强化学习概念  
通过与环境的交互学习最优决策，Reward是强化学习的核心概念之一，它是指环境给予智能体的反馈信号，用于衡量某个动作的好坏程度。


![image](https://github.com/user-attachments/assets/8cf90c55-5c05-499d-86ad-92921dbd08b7)

# PPO 
依赖于一个与策略模型大小相当的价值网络来估计函数。这个价值网络需要在每个时间步对状态进行评估，计算复杂度高，内存占用大。  
使用广义优势估计（GAE）来计算优势函数，需要对每个动作的即时奖励和未来奖励的折扣总和进行估计。  
优点：每次训练只能小幅调整，稳定可控，适合复杂任务
缺点：需依赖“评估师”（Critic网络）和大量训练数据，计算成本高。

# DPO
主要用于从人类偏好数据中直接学习策略。与PPO不同，DPO不依赖于传统的奖励函数，而是通过直接优化策略来匹配人类的偏好。  
优点：无需训练奖励模型，显存占用低，适合快速微调。  
缺点：依赖高质量偏好数据（若老师批改不准确，学生可能学偏）。

# GRPO
完全摒弃了价值网络，通过组内相对奖励来估计优势函数。这种方法通过比较同一状态下的多个动作的奖励值来计算相对优势，显著减少了计算和存储需求。  
通过采样一组动作并计算它们的奖励值，然后对这些奖励值进行归一化处理，得到相对优势。这种方法更直接，减少了对复杂奖励模型的依赖。  
优点：显存占用降低，适合资源受限场景；多候选生成增强多样性。  
缺点：推理时需生成多个候选答案，耗时增加。


训练数据格式  
- PPO  

  
- DPO  
![image](https://github.com/user-attachments/assets/2a5e097e-5da7-4967-b8a6-68eb7e1431b3)

  
- GRPO
![image](https://github.com/user-attachments/assets/fff628a0-e04e-4b62-8633-fb675be9cda8)

```python
import re
from datasets import load_dataset

SYSTEM_PROMPT = """Respond in the following format:
<reasoning>
  ...
</reasoning>
<answer>
  ...
</answer>"""

def extract_xml_answer(text: str) -> str:
    return text.split("<answer>")[-1].split("</answer>")[0].strip()

def extract_hash_answer(text: str) -> str | None:
    return text.split("####")[-1].strip() if "####" in text else None

def get_gsm8k(split: str = "train"):
    ds = load_dataset("openai/gsm8k", "main", split=split)

    def _map(example):
        return {
            "prompt": [
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": example["question"]},
            ],
            "answer": extract_hash_answer(example["answer"]),
        }

    return ds.map(_map)
```
