## 对attention的理解
核心思想：在处理序列时，应该关注输入中重要的部分，通过学习不同部分的权重，将输入序列中重要的进行加权，从而达到这个目的。

## 注意力计算步骤
传统注意力
$$
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V
$$

## attention和全连接的区别
全连接没有qkv的概念，当输入为v的时候，输出就是v的加权。attention中使用query作为锚点，注意力分数就是计算锚点的距离得到的。  
例：我们要从图书馆里找一本书，你只记得大概的内容，然后每本书都要花时间去浏览，这就是全连接，而attention则是告诉你一些关键信息，例如封面的颜色，书名里包含“AI”关键词。

##  目前主流的attention方法
