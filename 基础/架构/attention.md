## 对attention的理解
核心思想：在处理序列时，应该关注输入中重要的部分，通过学习不同部分的权重，将输入序列中重要的进行加权，从而达到这个目的。

## 注意力计算步骤
传统注意力
$$
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V
$$

## 
