# 词向量是对自然语言进行编码，转换为嵌入空间表示的工具。

- 词向量可以通过使用大规模语料进行无监督学习训练得到，早期的算法有CBOW连续词袋模型和skip-gram跳字模型。CBOW适合小规模训练语料，基于上下文预测中心词。skip-gram适合大规模训练语料，可以基于滑窗随机选择上下文词语。word2vec模型训练时默认采用skip-gram。

- now，先进的embedding模型代表：bge-m3、Qwen3-Embedding模型，他们基于大语言模型有监督训练得来，训练数据包括“指令、查询、正例文档和负例文档”。

- Qwen3-Embedding模型基于Qwen3基础模型的稠密版本构建，因为其预训练阶段已积累了强大的文本建模能力
- 输入格式：将指令与查询拼接在一起，形成“{Instruction}{Query}<|endoftext|>”的输入，文档则保持原样。
- 向量提取：在输入序列末尾添加[EOS]标记，模型处理后，取[EOS]对应的最后一层隐藏状态作为最终Embedding向量。这一步如同从一篇文章的结尾提炼核心观点，简洁而精准。
```
[EOS]的详细解释

因果注意力机制的约束
在 Transformer 等模型中，注意力要求每个 token 只能关注其左侧（前面）的 token，不能看到右侧（后面）的内容，因为会在decoder使用mask。以序列 “ABC [EOS]” 为例：
处理 A 时，只能关注自身；
处理 B 时，能关注 A 和 B；
处理 [EOS] 时，能关注 A、B、C 及所有前面的 token。
因此，[EOS] 是唯一能全局感知整个输入序列的位置，其隐藏状态包含了从指令到查询的全部语义信息。

语义完整性
若取中间某一 token 的隐藏状态，其向量仅包含局部信息，无法体现指令（如 “判断语义相似性”）的引导作用。而 [EOS] 的向量整合了 “指令 + 查询” 的完整语义，例如：
输入 “判断语义相似性苹果”，[EOS] 向量会编码 “苹果” 的语义，同时包含 “需要计算语义相似度” 的任务信息。
```
