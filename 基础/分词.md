# 在大型语言模型中，分词是一个非常重要的步骤。无论是处理中文、英文或者其他语言，模型都需要将输入文本分割成有意义的单元（tokens），以便进行后续的处理和学习。这个过程通常被称为“tokenization”

## 难点
中文分词不像英文那样，天然有空格作为分隔。而且中文词语组合繁多，分词很容易产生歧义。因此中文分词一直以来都是NLP的一个重点，也是一个难点。难点主要集中在分词标准，切分歧义和未登录词三部分。

## 分词算法
基于统计的方法：
- HMM，隐马尔科夫模型。
- CRF
- 深度学习
- BPE/BBPE
目前最主流的就是BPE，基于字节级的统计方法。优势在于BPE不需要构造专门的数据集以及缓解未登录词的问题
```text
BPE 的工作原理
1.初始化：通常从字符级别的词汇表开始，把每一个字符都视为一个token
2.统计对频率：在语料库中，统计所有相邻字符对出现的频率
3.合并出现频率高的对：找到出现频率最高的字符或者字词对，将这对字符/子词加入到词汇表
4.重复步骤2和3直到达到预设的词汇表大小
```

