# 大模型发展史
> 调侃为Google和OpenAI的军备竞赛

                2017年，Google 机器翻译团队发表的《Attention is All You Need》，也就是transformer，开山之作，引出encoder和decoder架构。

                2018年6月，OpenAI 发表了《Improving Language Understanding by Generative Pre-Training》即GPT-1，参数规模为1.17亿。仅采用decoder部分。

                2018年10月，Google发表了《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》，即Bert，参数规模为1.1亿-13亿。采用encoder部分进行特征抽取。在大量的NLP任务上优于GPT。

                2019年2月，OpenAI发表了《Language Models are Unsupervised Multitask Learners》，即GPT-2，参数规模15亿，性能与Bert相当。

        里程碑：2020年6月，OpenAI发表了《Language Models are Few-Shot Learners》，即GPT-3，参数规模为1,750 亿，此时能力“涌现”现象初露锋芒。

                2021年7月，OpenAI发表了《Evaluating Large Language Models Trained on Code》，即Codex

                2022年，OpenAI发布《Training language models to follow instructions with human feedback》即instruct-GPT

        里程碑：2022年底，基于GPT-3.5，ChatGPT上线，成为现象级产品。
